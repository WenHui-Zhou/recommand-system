[TOC]

# 自然语言处理

## 语言模型简介

每一门语言中所有可能的句子服从某一个分布，每个句子出现的概率为1，那么语言模型的任务就是预测每个句子在语言中出现的概率。对于语言模型中，一个好的语言模型因得出相对高的概率。相当于在给定输入的情况下，对目标语言的下一个句子进行估算。模型通常采用最大似然估计。

## 语言模型的评价方法

语言模型通常通过刻画复杂度来完成：

<img src = "../images/nlp_1.png">

该指数可以认为是平均分支系数，即模型预测下一个词的平均可选择数量。使用交叉熵损失来计算网络输出的损失：

```python
tf.nn.sparse_softmax_cross_entropy_with_logits()
tf.nn.softmax_cross_entropy_with_logits() # 输入是概率值
```

由于计算机难以处理文本数据，需要将文件数据转化成单词序列，然后为每一个单词分别映射一个0-9999之间的整数编号，然后将词汇表保存到一个独立的vocab中。下面是代码部分：

```python
import codec
import collections
from operator import itemgetter

raw_data = './data.train.txt'
vocab_output = 'ptb.vocab'
counter = collections.Counter()
with codec.open(raw_data,'r','utf-8') as f:
  for line in f:
    for word in line.strip().split():
      counter[word]+=1
# 词频排序
sorted_word_to_cnt = sorted(counter.items(),key=itemgetter(),reverse=True)
sorted_words = [x[0] for x in sorted_word_to_cnt]
sorted_words = ["<eos>"] + sorted_words
with codes.open(vocab_output,'w','utf-8') as file_output:
  for word in sorted_words:
    fiel_output.write(word + '\n')
```

随后将原始文本数据中的单词替换成编号，得到word_to_id的一张大表，然后保存成TFRecord文件。供之后使用。



