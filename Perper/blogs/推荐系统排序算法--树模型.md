[TOC]

摒弃浮浅工作

# 树模型

树模型包括了决策树、随机森林、GBDT、深度森林等。树模型的优点是可以通过有监督的方式，进行特征的自动交叉和选择。是机器学习中的经典算法，既可作分类模型也可以做回归模型。

## 决策树算法

决策树是一个树结构，节点有两种类型，内部节点为特征或属性，叶子节点为类别。

决策树的核心为：**从训练数据中归纳出一组分类规则，采用局部最优的方法，逐步达到。**

决策树学习通常分为三个步骤：特征选择、决策树生成、决策树剪枝

### 特征选择

特征选择（即分裂的条件，如是否为学生）的原则是采用局部最优，使得分类到同一组的数据更加的有序。纯度越高则分裂效果越好。

评价这种纯度的指标有熵、基尼指数、方差

#### 熵

熵是一个物理概念，指的是混乱程度。当一个发生概率很高的事情发生的时候，我们习以为常，这件事没有什么信息量（太阳升起）。但是当一件发生概率很低的事情发生的时候，对我们的冲击是很大的（发生地震）。熵的表达式如下：
$$
I_e = -\log_2p_i
$$
图像处在log上升曲线0-1之间。

我们终极目标是最求熵最小，事情最可预测，系统最稳定的状态。（这种稳定的收益是否存在一个上界呢，答案是显然的。人心是不安分的，事情是一直在变化之中的。）

因此我们希望在树的分裂过程中，分裂前后熵减最大，即分裂后更加的稳定。于是现在面临的问题转化为如何描述每个事件的熵、以及如何找到熵减最大的分裂。

一个事件是否发生以及他本身的信息量决定了这个事情带来的冲击程度（中彩票和大地震的影响是不同的），因此这个事件的信息熵可以表示为：
$$
H(x) = -\sum_{i=1}^{n}p(x_i)log_2p(xi)
$$
即发生概率与本身的熵的乘积。

#### 信息增益

如何确定熵减最大呢，即当前节点，我选择特征A最为分裂的条件，分裂之后树的熵与分裂之前树的熵的差值：
$$
g(D,A) = H(D) - H(D|A)
$$
H(D|A)即特征A作为划分条件，得到的信息熵，计算方式为：
$$
H(D|A) = \sum \frac{|D_i|}{|D|}H(D_i)
$$
即选中样本的概率乘上样本具有的熵。

信息增益有一个缺点，即他倾向于分类多的那一个属性。即每个样本的id的独有的，信息增益会选择这个属性，因为得到的样本最稳定，但是对我们的归类任务来说，将每个样本划为一类没有意义，因此我们需要进行改进：信息增益比。

#### 信息增益比

由于分裂本身也会带来熵的变化，当分类越细的时候，带来的分裂信息熵也就越大（分类越细每一个样本属于某一类的概率越小），分裂信息熵表示为：
$$
split_{info}(A) = -\sum \frac{|D_j|}{|D|} \log_2 \frac{|D_j|}{|D|}
$$
信息增益比既顾及到分裂后的熵，也估计到分裂过程中分裂的分支树少：
$$
g_R(D,A) = \frac{g(D,A)}{split_{info}(
A)}
$$

#### 基尼系数

基尼系数从另一个角度出发，对不纯度进行衡量，认为不纯度越高基尼系数越高。他的表达式是类别概率乘以错分概率：
$$
jini = \sum_{i=1}^{C} f_i(1-f_i)
$$
通常认为对一个二分问题来说，0.5是分类错误的下限（乱选的概率）。因此此时基尼系数最大。当概率越高，错分机会越小，基尼系数也越小。

#### 方差

方差也是衡量纯度的指标：
$$
\frac{1}{N} \sum_{i=1}^{N} (y_i - u)^2
$$
u为所有样本的均值，y是某个实例的标签，方差越大数据越是不纯。

### 决策树的生成

生成方式即分裂原则的选择方式，不同选择方式的决策树称呼也不一样。

#### ID3 信息增益分裂准则

ID3在进行分裂的时候，选择产生最大的信息增益的特征作为分裂的标准。

#### C4.5 信息增益比分裂准则

C.4.5是ID3的改进版本，选择信息增益比作为分裂的准则，这样做避免了多分类的倾向（选择id）。

### 决策树的剪枝

决策树由于分裂规则的限制，容易发生过拟合的现象，因此要对树进行剪枝。

剪枝的目标是达到预测误差和树复杂度之间的平衡。

预测误差是所有叶节点经验熵之和：
$$
C(T) = \sum N_tH_t(T)
$$
数复杂度即节点的数量，因此最后表示为：
$$
C_a(T) = C(T) + \alpha|T|
$$
剪枝的原则就是保证上式最小。方法是比较剪枝前后的C值，选择较小的C值的方案。

## 决策树的集成学习

在实际应用中，决策树方法常用于集成学习。最常见的是随机森林和GBDT算法。

#### Bagging 方法

决策树由于分离的原则是固定的，因此得到的决策结果也是固定的，难以得到提升。如何对这个结果进行改进呢，可以利用多棵树进行预测，然后投票产生结果。（我们作为先知，觉得很多想法非常简单，不值得一提。原因是这个问题不是直接抛向你的）

bagging方法将数据采样成N份，每一份训练一个决策树，当新来一个物品，利用N个决策树共同决策，最终的结果投票产生。

**随机森林**采用的表示这种方法，每棵树的决策权重相同。

#### Boosting 方法

跳出这种约束，由于数据唯一决定了树，我们还能想什么方法呢。再一次作为先知，我们说可以增强错分样本的权重，为每一课决策树赋予决策的重要性。

boosting的主要思路是：

1. 首先采集N个样本训练第一个分类器
2. 计算该分类器的错误率e
3. 根据错误率计算这个分类器的可信度（投票权重），以及更新采样权值D（错误样本重复采样概率大）
4. 训练第二个分类器，重复上述步骤，直到训练N个分类器
5. 根绝每个分类器的可信度加权进行投票

**GBDT是adaboost家族的成员，正是采用这样的方法来训练网络的。**

#### GBDT实现细节

对GBDT算法的详细介绍：https://ranmaosong.github.io/2019/04/27/ML-GBDT/

GBDT生成树的过程：

1. 从深度为 0 的树开始，对每个叶节点枚举所有的可用特征
2. 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）CART
3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集

选择分裂点利用了CART的思想，计算收益用到了Boosting的思想（负梯度方向优化）



## 代码实现

### 实验准备



### 代码思路



### 实验结果



