## 第一章

推荐系统：利用各类历史信息，猜测其可能喜欢的内容。

信息包含：物品信息、用户信息、场景信息

推荐系统涉及到大量的数据处理，系统工程。在处理数据是，需要考虑到：

1. 选用什么数据
2. 如何存储数据
3. 如何更新数据
4. 如何做数据的预处理

推荐系统模型主要由两层，分别是召回层和排序层。召回层主要负责从海量的数据中筛选用户可能感兴趣的若干条信息。精排阶段则是对筛选出的信息进行排序。

模型部分分为：模型离线训练和在线更新。

离线训练：逼近全局最优，使用全局数据

在线更新：实时消化新的数据样本，能够反映新数据的变化趋势

## 第二章



### 深度系统的进化之路

在2010年之前，推荐系统是千篇一律的协同过滤，从物品相似度的角度出发即ItemCF，从用户的角度出发即UserCF，后面在协同过滤的基础上衍生出来的矩阵分解模型MF，进一步提升了模型的泛化能力。



### UserCF

相似的用户感兴趣的东西都是相似的，因此可以推荐策略可以为相似用户购买（点击）的物品。

它的思路是根据用户是否购买物品，列一个**共现矩阵**，每一个作为i，j表示用户i与物品j之间的关系。因此用户可以通过购买行为表示成一个一维的数组。

计算相似用户。计算相似度的策略可以是**余弦相似度**、**皮尔逊相关系数**等。
$$
sim（i,j）= cos(i,j) = \frac{i·j}{||i||·||j||}
$$
对目标用户找到topN相似的用户之后。从topN用户中找出目标用户没有发生行为的item，然后准备推荐给目标用户。

需要对想要推荐的物品进行一个排序，排序的原则是，用户相似性与用户对物品喜好程度的乘积：
$$
R_{u,p} = \frac{\sum_{s\epsilon S}(W_{u,s}·R_{s,p})}{\sum_{s\epsilon S} W_{u,s}}
$$
得分越高的排在越前面。

UserCF通常难以应用，主要原始有：

1. 用户数量过多，存储用户与物品的喜好表将造成巨大的开销
2. 用户的历史记录（购买的商品）往往十分稀疏，找到的相似用户准确度十分低

### itemCF

itemCF的思想是物品之间存在相似性，当我购买一项物品的时候，我可能也会购买与之相似的物品。

首先构造物品相似矩阵：每个物品都有与之对应的购买人群，通过计算两个物品i,j之间共同购买的人比上所有购买的人的比，作为物品之间的相似度：
$$
W_{i,j} = \frac{|N(i)|\bigcap|N(j)|}{\sqrt{|N(i)|\bigcup|N(j)|}}
$$
得到物品之间的相似度矩阵之后，根据目标用户的购买历史中的物品，找出相似度最大的物品，排序后作为推荐的对象。如果多个物品产生的相似性需要进行累加。



### UserCF与itemCF的应用场景

UserCF基于用户相似，具有很强的社交特性，适合一些新闻推荐具有实时性，热点性的信息。

itemCF则更加适合一些兴趣较为稳定的应用，例如商品或电影的推荐。

### 协同过滤的不足

1. 协同过滤的共现矩阵进行体现了两个物品之间的联系，而不能进行推广到多种（即只表现为两个物品同时出现），因此内在信息的表达能来有限。容易导致热门物品出现概率高，容易和大量物品产生相似性。但冷门物品出现很少（数组很稀疏），通常难以推荐。（因此矩阵分解被提了出来。）
2. 协同过滤仅仅考虑到了用户和物品的交互行为（是否购买，是否出现）。未考虑到用户以及物品自身的各种信息，甚至是环境信息，因此以逻辑回归为核心的，能够综合各类特征的推荐系统称为主流。



### 协同过滤的进化--矩阵分解算法

由上可知，共现矩阵是商品两两之间的交互行为的体现。缺少多种商品之间的交互，一个直观的想法就是，对共现矩阵进行矩阵分解。将共现矩阵转化为用户矩阵和物品矩阵的点乘积。此时用户矩阵和物品矩阵称隐向量。用公式表示如下：
$$
R = U · V
$$
其中U为用户矩阵：m x k，V为物品矩阵大小为k x n。其中k为隐向量长度。

对于某个用户u和物品i来说，他们的预估评分（共现值）是：
$$
r_{ui} = q_ip_u
$$
矩阵分解的手段有很多，常见的有**特征值分解**（需要方阵，大多数情况不适用）。

**奇异值分解**：
$$
M = U \Sigma V^{T}
$$
其中U，V都是正交方阵（$A·A^{T} = E$），$\Sigma$ 为对角阵。

对对角阵进行从打到小排序，选择topK，同时选择U，V中相对应的向量，因此得到用户和物品的隐向量，完成分解。

**奇异值分解的缺点：**

1. 要求原始矩阵是稠密的，如果矩阵过于稀疏，效果不好
2. 计算的复杂大太大:$mn^2$

因此用的也不多。

**利用梯度下降获得隐向量**

目标函数为：
$$
\min_{q,p} \sum_{(u,i)\epsilon K}(r_{ui} - q_i·p_u)
$$
最小化转换后的隐向量的乘积与原始共现矩阵值的差距，达到近似转换的目的。通常为减小过拟合还会加入q,p的正则项。

相较于协同过滤来说：

1. 隐向量包含了全局的信息，具有更好的泛化性
2. 空间复杂度低，不用存太大的矩阵
3. 具有更好的扩展性和灵活性，方便与其他特征直接拼接（类似embedding）

### 基于逻辑回归的推荐流程

相较于共现矩阵，逻辑回归额外引入了用户、物品、场景的多种不同的特征。利用sigmoid做一个二分类。二分类的目标是判断一推荐用户点击的概率。概率值作为信息的评分。

通过一个感知层，对不同的信息赋予权重之后，传入sigmoid函数之中：
$$
sigmoid = \frac{1}{1+e^{-z}}
$$
利用梯度下降法最优化该式。



从训练过程中可以看出来，对特征进行转配之后需，即进入LR模型中，缺少对特征进行必要的交叉与筛选的步骤，使得模型的表达能力不强。因此大趋势是向着特征交叉而去。



## 特征交叉

高维的交叉特征中含有大量的信息。为了使得模型表达能力更强，特征交叉成为了一个很重要方向。算法工程师通常的做法是手动组合特征，随后再通过各种分析手段筛选特征。

### POLY2

poly2的思路是对所有特征进行暴力组合，类似于for循环，所有特征都进行两两相交。
$$
POLY2 = \sum_{j_1=1}^{n-1}\sum_{j_2 = j_1 + 1}^{n}w_h(j_1,j_2)x_{j_1}x_{j_2}
$$
并为每个交叉特征都赋予了权重。他的缺陷是：

1. 特征交叉后更加的稀疏了（one-hot），导致大部分特征难以有效训练
2. 权重参数数量由n上升到n平方，增大了训练的难度。

### FM模型

为解决上述稀疏特征难以训练、交叉学习的问题。FM为此而提出。

思路类似于MF，MF对共现矩阵进行矩阵分解。FM模型对POLY2的参数进行矩阵分解：
$$
FM = \sum_{j_1=1}^{n}\sum_{j_2 = j_1 + 1}^{n}(w_{j_1}w_{j_2})x_{j_1}x_{j_2}
$$
这个过程将参数量由$kn^2$降为$kn$。

内积的方式有效解决了数据稀疏的问题，例如A-C从未出现过，但A-B,B-C出现过，通过内积的方式，可以得到A-C。一个从来出现过的特征也可以通过组合来学习到。

相比于poly2，FM丢失了某些具体特征组合的精确记忆，但泛化性大大提高。

### FFM模型

Field FM通过引入特征域知识，使得模型表达能力更强。

与FM的区别是，同一个特征，FM对不同的域使用同一个隐向量，共kn个。而FFM对不同的域都对应着一条隐向量，共n·k·f个向量。

但FM模型的局限性在于，交叉的复杂度只能局限于二阶，高阶的交叉复杂度太高。因此如何突破二阶特征交叉的限制，进一步加强特征组合，成为了推荐系统发展的方向。

### GBDT + LR

如何高效而且自动的对特征进行组合。一个有效的解决方案是GBDT + LR。

GBDT对特征进行筛选和组合，从而生成新的离散向量。最后将这个离散向量作为LR的输入，预测CTR。多层结构则是对特征进行有效的自动组合。

GBDT有多棵树组成，预测的方式是将所有子树的结果加起来。GBDT是通过注逐一生成决策子树的方式，生成整个森林。新的子树用于学习样本标签与当前树林预测值之间残差。

GBDT的特点：

1. 每一次分裂就是一次特征的组合，但同时丢失了大量的数值特征。
2. 样本的特征表示用子树的叶子节点来表示，例如0001，说明该样本落到了第四个叶子节点上。
3. GBDT + LR是特征自动化的开端。



### LS-PLM 阿里巴巴曾经主流推荐模型

LS-PLM是对逻辑回归自然的推广，它的思路是先对样本进行分类，然后对分类的结果分别进行回归，排除无关场景无关用户对模型的影响。
$$
f(x) = softmax(x)sigmoid(x)
$$
LS-PLM的优点：

1. 端到端的学习能力，softmax分类过程可以挖掘出数据中蕴含的非线性模式。
2. 模型的稀疏性强：LS-PLM在建模的时候，引入L1,L2范数，存在大量的0向量，因此仅需使用权重非零的向量，大大减小参数的数量，提升线上的推理速度。易于部署，泛化性强。



### 总结

在深度学习进入推荐系统之前，我们可以发现，推荐算法主要做的事情，就是加强特征的交叉。首先是共现矩阵（userCF,itemCF,MF），单纯利用用户和商品之间的交互行为（userCF,itemCF），随后是利用了全局商品的信息（MF）。

为了进一步利用物品、用户、环境的信息，逻辑回归模型被提出sigmoid，即通过特征处理得到一个特征向量，将这个特征向量传入sigmoid中，作为二分类，判断是否点击的概率，以概率作为排序的依据。

有了LR模型，前面的特征交叉就有了更大的自由度，于是YOLY2出现了，暴力的列举所有的特征进行交叉。FM对二阶交叉的参数进行矩阵分解，使得模型参数变小kn，具有更强的泛化性。FFM在FM的基础上，增加了领域特征，使得每一个特征对每一个域均有一组向量。所以有knf个参数。

FM类模型做交叉由于限于算力的影响，因此只能做到二阶交叉。为了兼顾效率以及交叉的阶数，GBDT+LR模型被提出。GBDT通过生成树的方式，学习样本与原有树林结果的残差，每棵GBDT树的深度表示特征交叉的阶数，阶数越高，特征交叉的程度越高。随后通过叶子节点得到编码后的结果（0001等）。

最后是阿里的推荐模型LS-PLM，秉承上面的思路，先对模型进行多分类（特征交叉的过程），随后一次对每一组分类进行LR模型的训练（得到w和b）。分类的过程避免了无关特征的干扰，对有效特征进行充分的交叉。



综上，深度学习工作来之前的工作，主要集中在特征交叉上，LR和特征交叉也为之后的算法打开了新的思路。







