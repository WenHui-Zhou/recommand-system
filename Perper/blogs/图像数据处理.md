[TOC]

# 图像数据处理

## TFRecord数据格式

当我们需要处理的数据复杂，我们需要有效处理输入数据的信息。tensorflow提供了TFRecord来同意数据的存储。

### TFRecord格式介绍

TFRecord文件的数据是通过`tf.train.Example Protocol buffer`来保存的，下面是`tf.train.Example`的定义：

```python
message Example{
  Features features = 1;
};
message Features{
  map<string,Feature> feature = 1;
};
message Feature{
  oneof kind{
    ByteList bytes_lst = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  };
}
```

Example 是一个数据结构比较简洁的数据，属性取值包含三类，例如图像可以解码成string，类别可以解码成int。下面是使用TFRecord对数据mnist进行保存：

```python
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np

def _int64_features(value):
    return tf.train.Feature(int64_list = tf.train.Int64List(value=[value]))

def _bytes_features(value):
    return tf.train.Feature(bytes_list = tf.train.BytesList(value=[value]))

mnist = input_data.read_data_sets(
    "./data",dtype=tf.uint8,one_hot=True
)
images = mnist.train.images

labels = mnist.train.labels
pixels = images.shape[1]
num_examples = mnist.train.num_examples

filename = './output.tfrecords'

writer = tf.python_io.TFRecordWriter(filename)
for index in range(num_examples):
    image_raw = images[index].tostring()
    example = tf.train.Example(features=tf.train.Features(feature={
        'pixels': _int64_features(pixel),
        'label' : _int64_features(np.argmax(labels[index])),
        'image_raw': _bytes_features(image_raw)
    }))
    writer.write(example.SerializeToString())
writer.close()
```

以下是读取tfRecord文件的代码：

```python
import tensorflow as tf
reader = tf.TFRecordReader()
filename_queue = tf.train.string_input_producer(
    ['./output.tfrecords']
)
_, seriialized_example = reader.read(filename_queue)
features = tf.parse_single_example(serialized_example,
           features = {
             'image_raw':tf.FixedLenFeature([],tf.string),
             'pixels': tf.FixedLenFeature([],tf.int64),
             'label': tf.FixedLenFeature([],tf.int64)
           })
image = tf.decode_raw(features['image_raw'],tf.uint8)
label = tf.cast(features['label'],tf.int32)
pixels = tf.cast(features['pixels'],tf.int32)

sess = tf.Session()
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess = sess,coord=coord)
for i in range(10):
  print(sess.run(image,label,pixels))
```



## 图像数据处理

tensorflow提供了图像处理函数，在图像增强时对图像进行处理。

### 图像编码处理

tf提供了图像读取以及编码的方法：

```python
import tensorflow as tf

image_raw_data = tf.gFile.FastGFile("./picture",'r').read()

with tf.Session() as sess:
  img_data = tf.image.decode_jpeg(image_raw_data)
# 上述代码将图片转化成三维的数值矩阵

encoded_image = tf.image.encode_jpeg(img_data)
with tf.gFile.GFile('output','wb') as f:
  f.write(encoded_mage.eval())
```

### 图像大小调整

tf提供了四种图像调整的方法，并封装到`tf.image.resize_images` 。

```python
img_data = tf.image.convert_image_dtype(img_data,dtype = tf.float32)

resized = tf.image.resize_images(img_data,[300,300],method = 0)
```

此外tf还提供了API对图像进行裁剪或填充：

```python
croped = tf.image.resize_image_with_crop_or_pad(img_data,1000,1000)
# 不够大就填充，太大就裁剪
```

按比例裁剪：

```python
central_cropped = tf.image.central_crop(img_data,0.5)
```

### 图像翻转

实现图像的上下左右翻转：

```python
# 上下翻转
flipped = tf.image.flip_up_down(img_data)
# 左右翻转
flipped = tf.image.flip_left_right(img_data)
# 对角线翻转
flipped = tf.image.transpose_image(img_data)
# 以一定的概率翻转
flipped = tf.image.random_flip_up_down(img_data)
flipped = tf.image.random_flip_left_right(img_data)
```

### 图像色彩调整

tf提供了许多颜色调整的函数：

```python
adjusted = tf.image.adjust_brightness(img_data,-0.5)
adjusted = tf.clip_by_value(adjusted,0.0,1.0)
# 提升亮度
adjusted = tf.image.adjust_brightness(img_data,0.5)
# 零度随机调整
adjusted = tf.image.random_brightness(image,max_delta)

# 调整对比度
adjusted = tf.image.adjust_contrast(img_data,0.5)

# 调整色相
adjusted = tf.image.adjust_hue(img_data,0.1)

# 饱和度
adjusted = tf.image.adjust_saturation(mg_data,-5)

# 图像标准化
adjusted = tf.image.per_image_standardization(img_data)
```

## 多线程输入数据处理框架

tf提供了一套多线程处理输入数据的框架，包含下面五个步骤：

- 指定原始数据的文件输出列表
- 创建文件列表队列
- 从文件中读取数据
- 数据预处理
- 整理成batch作为神经网络输入

### 队列与多线程

在tf中，队列与变量类似，都是计算图上有状态的节点。其他计算节点可以修改他们的状态。对于队列，修改队列的操作主要由enqueue，enqueuemany，dequeue 三种，下面程序展示如何使用这些函数来操作队列：

```python
import tensorflow as tf

q = tf.FIFOQueue(2,"int32")

init = q.enqueue_many(([0,10],))

x = q.dequeue()

y = x + 1
q_inc = q.enqueue([y])

with tf.Session() as sess:
    init.run()
    for _ in range(5):
        v, _ = sess.run([x,q_inc])
        print(v)
```

另一种队列是`tf.RandomShuffleQueue`，这种队列操作得到的是从当前队列所有元素中随机选择一个。在训练神经网络的时候，使得使用的训练数据尽量的随机。

队列不仅仅是一种数据结构，还是一种异步计算算张量取值的一个重要机制，比如有多个线程可以同时向一个队列中写元素，或者同时读取一个队列中的元素。

**tensorflow提供了`tf.Coordinator,tf.QueueRunner` 两个类来完成多线程协同功能。**

其中`tf.Coordinator` 主要用于协同多个线程一同停止，它提供了三个函数，分别是`should_stop, request_stop, join` 。 当某一个线程调用`request_stop` 之后，should_stop就会被设置为true，使得其他所有的线程都停止：

```python
import tensorflow as tf
import numpy as np
import threading
import time

def Myloop(coord, worker_id):
    while not coord.should_stop():
        if np.random.rand() < 0.1:
            print('stopppping from id %d \n'%worker_id)
            coord.request_stop()
        else:
            print("working on id %d\n"%worker_id)
        time.sleep(1)

# 声明一个tf.train.Coordinator
coord = tf.train.Coordinator()

threads = [
    threading.Thread(target=Myloop, args = (coord, i,)) for i in range(5)
]
for t in threads:
    t.start()

coord.join(threads)
```

如上述代码，当所有的线程执行一遍的时候，会暂停1s，然后开始第二轮的执行，知道调用了request_stop，然后所有线程停下来，执行到一半的线程继续执行完。

`tf.QueueRunner` 主要用于启动多个线程来操作同一个队列，然后通过`tf.Coordinator` 来管理。

如下代码：

```python

import tensorflow as tf
queue = tf.FIFOQueue(100,'float')
enqueue_op = queue.enqueue([tf.random_normal([1])])

# 启动5个线程,针对queue，执行enqueue操作
qr = tf.train.QueueRunner(queue,[enqueue_op]*5)
# 加入默认集合
tf.train.add_queue_runner(qr)
out_tensor = queue.dequeue()

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess,coord=coord)
    for _ in range(5):
        print(sess.run(out_tensor)[0])
    coord.request_stop()
    coord.join(threads)
```

### 输入文件队列

本节将通过介绍如何使用tf中的队列管理输入文件列表。假设所有输入数据都整理成`TFRecord`，我们将数据分成多个TFRecord文件来提高处理效率。

tf提供了`tf.tran.match_filenames_once` 来通过正则项获取所有的文件。提供 `tf.train.string_input_producer` 使用提供的文件列表创建一个输入队列，这个输入队列可以作为文件读取函数的参数。**该函数生成的输入队列可以同时被多个文件读取线程操作，而且输入队列会将队列中的文件均匀地分给不同的线程，不出现有些文件被处理过多次，而有些文件还没有被处理过的情况。** 通过设置num_epochs 参数来限制文件读取的次数。

随机生成TFRecord数据：

```python
import tensorflow as tf
def _int64_feature(value):
  return tf.train.Feature(int64_list = tf.train.Int64List(value=[value]))
num_shards = 2
instances_per_shard = 2
for i in range(num_shards):
  filename = ('./data.tfrecords-%.5d-of-%.5d'%(i,num_shards))
  writer = tf.python_io.TFRecordWriter(filename)
  for j in range(instances_per_shard):
    example = tf.train.Example(features = tf.train.Features(feature={
      "i":_int64_feature(i),
      "j":_int64_feature(j)
    }))
    writer.write(example.SerializeToString())
    writer.close()
```

读取数据文件：

```python
import tensorflow as tf
files = tf.train.match_filenames_once('./data.tfrecords-*')
# 生成文件队列
filename_queue = tf.train.string_input_producer(files,shuffle=False)
reader = tf.TFRecordReader()
_,serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
serialized_example,
features = {
'i':tf.FixedLenFeature([],tf.int64),
'j':tf.FixedLenFeature([],tf.int64),
}
)
with tf.Session() as sess:
tf.local_variables_initializer().run()
print(sess.run(files))
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess,coord = coord)
for i in range(6):
print(sess.run([features['i'],features['j']]))
coord.request_stop()
coord.join(threads)
```

### 组合训练数据

我们使用文件队列来管理文件，随后通过`tf.train.batch` 或者`tf.train.shuffle_batch` 函数来将单个样例组织成batch。

```python
example,label = tf.train.batch([example,label],batch_size = batch_size,capacity = capacity)
```

**一个简单的输入数据处理框架**

创建文件队列：

```python
files = tf.train.match_filenames_once('./data-*')
filename_queue = tf.train.string_input_producer(files,shuffle=False)
```

读TFRecord文件：

```python
reader = tf.TFRecordReader()
_,serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
  serialized_example,
  features = {
    'image':tf.FixedLenFeature([],tf.string),
    ...
  })
image,label =features['image'],features['label']
...
decoded_image = tf.decode_raw(image,tf.unit8)
...
# preprocess
```

将数据组织成一个batch：

```python
image_batch,label_batch = tf.train.shuffle_batch(
  [decoded_image,label],batch_size=batch_size,capacity=capacity,
  min_after_dequeue = min_after_dequeue
)
```

正向传播的过程：

```python
logit = inference(image_batch)
loss = calc_loss(logit,label)
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
```

多线程部分：

```python
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess = sess,coord = coord)
# 迭代循环
...

# 停止线程
coord.request_top()
coord.join(threads)
```

### 数据集（Dataset）

tensorflow提供了一套更加高层的数据处理框架，能够方便进行batching，shuffle等操作。

下面是一个简单的例子：

```python
import tensorflow as tf

input_data = [1,2,3,5,8]
dataset = tf.data.Dataset.from_tensor_slices(input_data)
iterator = dataset.make_one_shot_iterator()
x = iterator.get_next()
y = x*x

with tf.Session() as sess:
    for i in range(len(input_data)):
        print(sess.run(y))
```

- tf.data.Dataset.from_tensor_slices()：表明数据集从一个张量中构建的
- dataset.make_one_shot_iterator()：遍历数据集
- iterator.get_next()：不将所有数据载入内存，而是使用迭代器每次生成一条记录

如果从text中获取数据集：

```python
input_files = ['path/to/input_file1','path/to/input_file2']
dataset = tf.data.TextLineDataset(input_files)
```

如果数据是按照TFRecord方式存储的，则需要提供一个专门的parser进行解析：

```python
import tensorflow as tf
def parser(record):
  features = tf.parse_single_example(
    record,
    features = {
      'feat1':tf.FixedLenFeature([],tf.int64),
      'feat2':tf.FixedLenFeature([],tf.int64),
    })
  return features['feat1'],featrues['feat2']

input_files = ['path/to/input_file1','path/to/input_file2']
dataset = tf.data.TFRecordDataset(input_files)
dataset = dataset.map(parser)
iterator = dataset.make_one_shot_iterator()
feat1,feat2 = iterator.get_next()
with tf.Session() as sess:
  for i in range(10):
    f1,f2 = sess.run([feat1,feat2])
```

另外，在我们不知道数据集大小的情况下，可以使用`dataset.make_initializable_iterator()`。

迭代的时候使用try，except来迭代数据集。

### 数据集的高层操作

```python
dataset = dataset.map(parser)
```

map方法对数据集上的每一条记录调用参数指定的parser方法，对每一天记录进行处理后，map函数将处理后的数据包装成一个新的数据集返回。

```python
# 也可以自定义预处理方法
dataset = dataset.map(lambda x: pretrained_train(x,image_size,image_size,None))
```

其他高级的方法：

```python
# 打乱数据集的记录
dataset = dataset.shuffle(buffer_size)
# 将数据组合成batch返回,通过get_next() 返回尺度为[batch_size,image_size,image_size]
dataset = dataset.batch(batch_size)
```

repeat将数据集重复N次：

```python
dataset = dataset.repeat(N)
```

```python
# 链接数据集
concatenate() 
# 跳过若干数据
skip(N)
# 读取数据集中的前N项
take(N)
```

下面是一个较完整的例子：

```python
import tensorflow as tf
train_files = tf.train.match_filenames_once('train_files-*')
def parser(record):
  features = tf.parse_single_example(
    record,
    features = {
      'feat1':tf.FixedLenFeature([],tf.int64),
      'feat2':tf.FixedLenFeature([],tf.int64),
      ...
    })
  decoded_image = tf.decode_raw(features['image'],tf.uint8)
  return features['feat1'],featrues['feat2'] ...
dataset = tf.data.TFRcordDataset(train_files)
dataset = dataset.map(parser)
dataset = dataset.map(lambda x: pretrained_train(x,image_size,image_size,None))
dataset =  dataset.shuffle(shuffle_buffer).batch(batch_size)
dataset = dataset.repeat()
iterator = dataset.make_one_shot_iterator()
image_batch,lable_batch = dataset.get_next()
...
logit = inference(image_batch)
loss = calc_loss(logit,label_batch)
train_step = tf.train.FradientDescentOptimizer(learning_rate).minimize(loss)
...
with tf.Session() as sess:
  sess.run(tf.all_variables_initializer())
  for i in range(100):
    sess.run(train_step)
...
```









